{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MACHINE LEARNING: Portugal Wine under Two Class approach for Quality classification with Logistic Regression using R and K fold cross-validation\n",
    "# Module 4: Conclusion\n",
    "\n",
    "By: Hector Alvaro Rojas | Data Science, Visualizations and Applied Statistics | October 25, 2017<br>\n",
    "Url: [http://www.arqmain.net]   &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;   GitHub: [https://github.com/arqmain]\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I used the wine data set from the UCI Machine Learning data repository. The goal is to predict wine quality which has 7 integer values from 3 to 9. \n",
    "\n",
    "The original data is separated into white and red datasets. I combined them and created one additional variable: \"Class\" indicating \"white\" or \"red\" wine. After that, I re-coded the \"quality\" variable to indicate scores greater than or equal to 6 (denoted as \"Good\") or to indicate scores smaller than 6(denoted as \"Bad\").\n",
    "\n",
    "This project develops Logistic Regression algorithm of machine learning to classify the quality of the wine \"Good\" or \"Bad\" according to the 12 variables that characterize the wine subject to classification.\n",
    "\n",
    "Our original dataset is an imbalance one. There are 6497 registers but only 2384 (36.7%) are of the Bad quality. So, I decided using the ROC metric to compare the model's performance.\n",
    "\n",
    "All the variables showed to have outliers. Some of them had a lot, some had a few. Mostly outliers were on the positive side of distributions.\n",
    "\n",
    "Along with all the above situations, I decided to use two datasets to develop the project. Dataset \"df\" use all registers.  The other one, \"dfffull\", consider all registers but replace outliers by a \"threshold\" value which is generated for each variable using the upper limit of its Box-plot.\n",
    "\n",
    "Other criterions that use information of variables in the dataset to help out us with the outlier problem may reach better solutions here. Anyway, the criterion used to generate the dfffull dataset is still better than eliminating outliers, because at least do not erase the information that lies in the other variables with no outlier presence at that data point.\n",
    "\n",
    "The modeling results were similar across all datasets. ROC metric was 80.41% for \"df\" and 80.55% for \"dfffull\"\n",
    "\n",
    "In each case (under \"df\" or \"dfffull\"), the model was developed selecting the variables included by using the stepwise backward elimination method based on AIC selection criterion. Anyway, this is modeling so, we may improve more or not the model by filtering other variables, incorporating variable's interactions or using another variable's selection method.\n",
    "\n",
    "Finally, whenever I work in some modeling project I try always to remember what the late brilliant George Box one time stated: <i><b>\" ... remember that all models are wrong; the practical question is how wrong do they have to be to not be useful?\"</b></i>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "By: Hector Alvaro Rojas | Data Science, Visualizations and Applied Statistics | October 25, 2017<br>\n",
    "Url: [http://www.arqmain.net]   &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;   GitHub: [https://github.com/arqmain]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
