{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MACHINE LEARNING: Portugal Wine under Two Class approach for Red and White classification with Logistic Regression using R and K fold cross-validation\n",
    "# Module 4: Conclusion\n",
    "\n",
    "By: Hector Alvaro Rojas | Data Science, Visualizations and Applied Statistics | October 30, 2017<br>\n",
    "Url: [http://www.arqmain.net]   &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;   GitHub: [https://github.com/arqmain]\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I used the wine data set from the UCI Machine Learning data repository. The goal is to predict wine class which can be \"white\" or \"red\". \n",
    "\n",
    "The original data is separated into white and red datasets. I combined them and created one additional variable: \"Class\" indicating \"white\" or \"red\" wine.\n",
    "\n",
    "This project develops Logistic Regression algorithm of machine learning to classify the class of the wine \"white\" or \"red\" according to the 12 variables that characterize the wine subject to classification.\n",
    "\n",
    "Our original dataset is an imbalance one. There are 6497 registers but only 1599 (24.6%) are of the red class of wine. So, we decided using the ROC metric to compare the model's performance. \n",
    "\n",
    "All the variables showed to have outliers. Some of them had a lot, some had a few. Mostly outliers were on the positive side of distributions.\n",
    "\n",
    "Along with all the above situations, I decided to use two datasets to develop the project. Dataset \"df\" use all registers.  The other one, \"dfffull\", consider all registers but replace outliers by a \"threshold\" value which is generated for each variable using the upper limit of its Box-plot.\n",
    "\n",
    "Other criterions that use information of variables in the dataset to help out us with the outlier problem may reach better solutions here. Anyway, the criterion used to generate the dfffull dataset is still better than eliminating outliers, because at least do not erase the information that lies in the other variables with no outlier presence at that data point.\n",
    "\n",
    "The modeling results were similar across all datasets. ROC metric was 99.66% for \"df\" and 99.78% for \"dfffull\"\n",
    "\n",
    "In each case (under \"df\" or \"dfffull\"), the model was developed selecting the variables included by using the stepwise backward elimination method based on AIC selection criterion. Anyway, this is modeling so, we may improve more or not the model by filtering other variables, incorporating variable's interactions or using another variable's selection method.\n",
    "\n",
    "Finally, whenever I work in some modeling project I try always to remember what the late brilliant George Box one time stated: <i><b>\" ... remember that all models are wrong; the practical question is how wrong do they have to be to not be useful?\"</b></i>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "By: Hector Alvaro Rojas | Data Science, Visualizations and Applied Statistics | October 30, 2017<br>\n",
    "Url: [http://www.arqmain.net]   &nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;   GitHub: [https://github.com/arqmain]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
